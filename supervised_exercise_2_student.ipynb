{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Our first classifier.\n",
    "\n",
    "\n",
    "## I. GOAL OF THE EXERCISE\n",
    "\n",
    "In this exercise you will practice the basic pipeline of the supervised learning task. Implement a simple classifier. And will try to solve several hinderances found in the process.\n",
    "\n",
    "## II. DELIVERABLES\n",
    "As you progress in this exercise, you will find several questions you are expected to answer them properly with adequate figures when required and deliver the notebook with the working code used for generating and discussing the results in due time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\" style=\"border-radius:10px\"> **IMPORTANT:** Write in the next cell the name of the people that answer this notebook\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Felix Altenberger, Pablo Reynoso, Emanuel Sanchez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. OUR FIST CLASSIFIER.\n",
    "We are given the data in diabetes.mat and our goal is to predict the whether a person suffers from diabetes or not given her medical record. Our first model to try is linear regression as explained in ”A gentle introduction to supervised learning”.\n",
    "\n",
    "### A. Understanding and preprocessing our problem.\n",
    "The first step in the learning pipeline is to have a general picture of your dataset particularities.\n",
    "\n",
    "### B. Data set analysis\n",
    "Load the dataset and describe the basic properties of the data,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**QUESTION BLOCK 1:**\n",
    "<ol>\n",
    "<li> Which is the cardinality (number of examples) of the training set?</li> \n",
    "<li> Which is the dimensionality of the training set? </li>\n",
    "<li> Which is the mean value of the training set? </li>\n",
    "</ol>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "data = sio.loadmat('diabetes.mat')\n",
    "\n",
    "X = data['x'].T\n",
    "y = data['y']\n",
    "#1.\n",
    "#print X.shape[0]\n",
    "#2.\n",
    "#print X.shape[1]\n",
    "#3.\n",
    "#print sum(X)/len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. The cardinality is 768.\n",
    "2. The dimensionality is 8.\n",
    "3. The mean values are [nan, nan, nan, nan, nan, nan, 0.4718763, 33.24088542]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are some missing values with value NaN and som\n",
    "e categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**QUESTION BLOCK 2:**\n",
    "<ol>\n",
    "<li> Create a new dataset D1, replacing the NaN values with the mean value of the corresponding attribute without considering the missing values. </li>\n",
    "<li> Create a new dataset D2, replacing the NaN values with the mean value of the corresponding attribute without considering the missing values conditioned to the class they belong, i.e. replace the missing attribute values of class +1 with the mean of that attribute of the examples of class +1, and the same for the other class. </li>\n",
    "<li> **[Optional :]** Explain another method to deal with missing values and apply it to preprocess the training data. Include the reference of the method used. Consider this new dataset as D3. </li>\n",
    "<li> Which are the new mean values of each dataset?</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#1. + 2.\n",
    "#declare vars\n",
    "means, ncount, means_classed, ncount_classed = [0]*8, [0]*8, [0]*8, [0]*8\n",
    "for i in range(8):\n",
    "    means_classed[i], ncount_classed[i] = [0,0],[0,0]\n",
    "    \n",
    "#calculate means\n",
    "for i in range(len(X)):\n",
    "    for j in range(len(X[0])):\n",
    "        if not math.isnan(X[i][j]):\n",
    "            means[j]+=X[i][j]\n",
    "            ncount[j]+=1\n",
    "            if y[i]==-1:\n",
    "                means_classed[j][0]+=X[i][j]\n",
    "                ncount_classed[j][0]+=1\n",
    "            elif y[i]==1:\n",
    "                means_classed[j][1]+=X[i][j]\n",
    "                ncount_classed[j][1]+=1\n",
    "for i in range(8):\n",
    "    means[i] = means[i]/ncount[i]\n",
    "    for j in range(2):\n",
    "         means_classed[i][j] = means_classed[i][j]/ncount_classed[i][j]\n",
    "  \n",
    "#create d1, d2\n",
    "d1,d2 = copy.deepcopy(X),copy.deepcopy(X)\n",
    "for i in range(len(X)):\n",
    "    for j in range(len(X[0])):\n",
    "        if math.isnan(X[i][j]):\n",
    "            d1[i][j] = means[j]\n",
    "            if y[i]==-1:\n",
    "                d2[i][j] = means_classed[j][0]\n",
    "            elif y[i]==1:\n",
    "                d2[i][j] = means_classed[j][1]\n",
    "#4.\n",
    "#print sum(d1)/len(d1)\n",
    "#print sum(d2)/len(d2)\n",
    "\n",
    "#just some code used for visualization. This is not needed for the assignment though\n",
    "#x0 = [row[0] for row in X]\n",
    "#plt.scatter(x0,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean values of d1 are:\n",
    "[4.49467275, 121.68676278, 72.40518417, 29.15341959, 155.54822335, 32.45746367, 0.4718763, 33.24088542]\n",
    "and the mean values of d2 are:\n",
    "[4.49265212, 121.69735767, 72.42814101, 29.24704236, 157.00352686, 32.44642005, 0.4718763, 33.24088542]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. A simple classifier\n",
    "\n",
    "Our first classifier is a thresholded regressor. Use and/or modify any of the methods you implemented for regression and apply it to find a linear classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**QUESTION BLOCK 3:**\n",
    "<ol>\n",
    "<li>In this model you have to learn the threshold value. Explain how you can accommodate this parameter.</li>\n",
    "<li>Report the normal vector of the separating hyperplane for each data set D1, D2, D3.</li>\n",
    "<li>Compute the error rates achieved on the training data. Are there significant differences? Report the method used and their parameters.</li>\n",
    "</ol>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code\n",
    "import numpy as np\n",
    "\n",
    "def ana_sol(x,y):\n",
    "    x_pinv = np.linalg.pinv(x)\n",
    "    w = np.dot(x_pinv, y)\n",
    "    return w\n",
    "\n",
    "def classify(w,x):\n",
    "    x_til = [1]*(len(x)+1)\n",
    "    x_til[1:] = x\n",
    "    return np.sign(np.dot(np.transpose(x_til),w))\n",
    "\n",
    "mat1 = np.array([[1]]*len(d1))\n",
    "x_til1, x_til2 = np.concatenate((mat1, d1), axis=1), np.concatenate((mat1, d2), axis=1)\n",
    "w1, w2 = ana_sol(x_til1,y), ana_sol(x_til2, y)\n",
    "#1.\n",
    "trs1, trs2 = -w1[0][0],-w2[0][0]\n",
    "#2.\n",
    "n1, n2 = w1[1:], w2[1:]\n",
    "#print n1,n2\n",
    "#3.\n",
    "j1, j2 = 0,0\n",
    "for i in range(len(y)):\n",
    "    if y[i] != classify(w1,d1[i]):\n",
    "        j1+=1\n",
    "    if y[i] != classify(w1,d2[i]):\n",
    "        j2+=1\n",
    "err1 = j1*1.0/len(y)\n",
    "err2 = j2*1.0/len(y)\n",
    "#print err1, err2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The threshold is learned by performing linear regression (using the analytical solution). -w[0] of the resulting vector w is the threshold that we are looking for.\n",
    "2. The normal vector of D1 is: [0.0454830899, 0.0128752133, -0.00276153057, 0.000309178172, -0.000175410884, 0.0273791130, 0.250953772, 0.00498452234] and the one of D2 is:\n",
    "[0.0556913, 0.00988663, -0.00151083, 0.01041739, 0.00186537, 0.0160551, 0.22947897, 0.00183327]\n",
    "3. The error rate on D1 is 0.22135 and the one on D2 is 0.21875. As expected, the error rate on D2 is lower, but surprisingly only by 0.00260, which is a very small amount.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training error is a poor estimation of the generalization error. Let us test what happens in a test set created by holding-out a certain percentage of the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**QUESTION BLOCK 4:**\n",
    "\n",
    "Repeat the learning process in block 3 using just D2 but holding-out the last fifth of the data set for testing purposes, i.e. use the first 4/5-th for training and the last 1/5-th for testing. Follow exactly the following steps in your process:\n",
    "<ol>\n",
    "<li> Clear your workspace: `%reset -f` at the begining of the cell. </li>\n",
    "<li> Preprocess the data replacing the NaN using the method for creating D2.</li>\n",
    "<li> Split your data in two sets: the first 4/5-th is to be used for training and\n",
    "the last 1/5-th will be used for testing purposes. Use a random seed value equal to 42.</li>\n",
    "<li> Train your model on the training set.</li>\n",
    "<li> Answer the following questions: Which is the error rate on your training\n",
    "data? Which is the error rate on your test data? Are they similar? Did you expect that behavior? Why?</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1.\n",
    "%reset -f\n",
    "\n",
    "#2. \n",
    "import scipy.io as sio\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import random as rd\n",
    "\n",
    "data = sio.loadmat('diabetes.mat')\n",
    "X = data['x'].T\n",
    "y = data['y']\n",
    "n = len(X)\n",
    "\n",
    "#declare vars\n",
    "means_classed, ncount_classed = [0]*8, [0]*8\n",
    "for i in range(8):\n",
    "    means_classed[i], ncount_classed[i] = [0,0],[0,0]\n",
    "    \n",
    "#calculate means\n",
    "for i in range(n):\n",
    "    for j in range(len(X[0])):\n",
    "        if not math.isnan(X[i][j]):\n",
    "            if y[i]==-1:\n",
    "                means_classed[j][0]+=X[i][j]\n",
    "                ncount_classed[j][0]+=1\n",
    "            elif y[i]==1:\n",
    "                means_classed[j][1]+=X[i][j]\n",
    "                ncount_classed[j][1]+=1\n",
    "for i in range(8):\n",
    "    for j in range(2):\n",
    "         means_classed[i][j] = means_classed[i][j]/ncount_classed[i][j]\n",
    "            \n",
    "#update values\n",
    "for i in range(n):\n",
    "    for j in range(len(X[0])):\n",
    "        if math.isnan(X[i][j]):\n",
    "            if y[i]==-1:\n",
    "                X[i][j] = means_classed[j][0]\n",
    "            elif y[i]==1:\n",
    "                X[i][j] = means_classed[j][1]\n",
    "\n",
    "#3. - randomly split data into test and training set\n",
    "split = int(round(n*0.8))\n",
    "X_train, X_test, y_train, y_test = [0]*split, [0]*(n-split), [0]*split, [0]*(n-split)\n",
    "rd.seed(42)\n",
    "split_count = 0\n",
    "used = [False]*n\n",
    "while split_count < split:\n",
    "    rand_val = rd.randint(0,n-1)\n",
    "    if not used[rand_val]:\n",
    "        used[rand_val] = True\n",
    "        X_train[split_count], y_train[split_count] = X[rand_val], y[rand_val]\n",
    "        split_count+=1\n",
    "split_count = 0\n",
    "for i in range(n):\n",
    "    if not used[i]:\n",
    "        X_test[split_count], y_test[split_count] = X[i], y[i]\n",
    "        split_count += 1\n",
    "        \n",
    "#4.\n",
    "def ana_sol(x,y):\n",
    "    x_pinv = np.linalg.pinv(x)\n",
    "    w = np.dot(x_pinv, y)\n",
    "    return w\n",
    "\n",
    "def classify(w,x):\n",
    "    x_til = [1]*(len(x)+1)\n",
    "    x_til[1:] = x\n",
    "    return np.sign(np.dot(np.transpose(x_til),w))\n",
    "\n",
    "mat1 = np.array([[1]]*len(X_train))\n",
    "X_til= np.concatenate((mat1, X_train), axis=1)\n",
    "w = ana_sol(X_til,y_train)\n",
    "\n",
    "#5.\n",
    "j_train, j_test = 0,0\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] != classify(w,X_train[i]):\n",
    "        j_train+=1\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] != classify(w,X_test[i]):\n",
    "        j_test+=1\n",
    "err_train, err_test = j_train*1.0/len(y_train), j_test*1.0/len(y_test)\n",
    "#print err_train, err_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error on the training set is 0.197068403909 and the one on the test set is 0.188311688312. The errors are very similar (the difference is only 0.00875671559711 (= ~4,7% decrease)). It is not surprising that the error rates are close together, since the complexity of the classifier is very low. According to Hoeffding's inequality, the out-of-sample-error should therefore be close to the in-sample-error, which is exactly what we observe. However we would have expected that the test error is higher than the training error, which is not the case. The reason for this should again be that the complexity of the classifier is very low, for which reason it is underfitting the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\"> **QUESTION BLOCK 5:**\n",
    "Repeat the process in block 4 changing the order of some of the steps. Follow exactly the following steps in your process:\n",
    "<ol>\n",
    "<li> Clear your workspace with `%reset -f`.</li>\n",
    "<li> Split your data in two sets: the first 4/5-th is to be used for training and the last 1/5-th will be used for testing purposes. Use random state or random seed value of 42. </li>\n",
    "<li> Preprocess the data replacing the NaN using the method for creating D2. But this time use only the data corresponding to the training set. </li>\n",
    "<li> Train your model on the training set.</li>\n",
    "<li> Replace the NaN values using the means computed on the training data. </li>\n",
    "<li> Answer the following questions: Which is the error rate on your training data? Which is the error rate on your test data? Are they similar? Did you expect that behavior? Why? </li>\n",
    "<li> Compare these results with the ones in block 4. Do we achieve better or worse results? Why?</li>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1.\n",
    "%reset -f\n",
    "\n",
    "#2.-7.\n",
    "import scipy.io as sio\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import random as rd\n",
    "\n",
    "data = sio.loadmat('diabetes.mat')\n",
    "X = data['x'].T\n",
    "y = data['y']\n",
    "n = len(X)\n",
    "\n",
    "def ana_sol(x,y):\n",
    "    x_pinv = np.linalg.pinv(x)\n",
    "    w = np.dot(x_pinv, y)\n",
    "    return w\n",
    "\n",
    "def classify(w,x):\n",
    "    x_til = [1]*(n+1)\n",
    "    x_til[1:] = x\n",
    "    return np.sign(np.dot(np.transpose(x_til),w))\n",
    "\n",
    "def calc_errs(split_perc):\n",
    "    #2\n",
    "    split = int(round(n*(split_perc/100.0)))\n",
    "    X_train, X_test, y_train, y_test = [0]*split, [0]*(n-split), [0]*split, [0]*(n-split)\n",
    "    rd.seed(42)\n",
    "    split_count = 0\n",
    "    used = [False]*n\n",
    "    while split_count < split:\n",
    "        rand_val = rd.randint(0,n-1)\n",
    "        if not used[rand_val]:\n",
    "            used[rand_val] = True\n",
    "            X_train[split_count], y_train[split_count] = X[rand_val], y[rand_val]\n",
    "            split_count+=1\n",
    "    split_count = 0\n",
    "    for i in range(n):\n",
    "        if not used[i]:\n",
    "            X_test[split_count], y_test[split_count] = X[i], y[i]\n",
    "            split_count += 1\n",
    "\n",
    "    #3. & 5. - we change the order of 4. and 5., because they do not interfer and it is much easier to do 3. and 5. together\n",
    "    #declare vars\n",
    "    means_classed, ncount_classed = [0]*8, [0]*8\n",
    "    for i in range(8):\n",
    "        means_classed[i], ncount_classed[i] = [0,0],[0,0]\n",
    "\n",
    "    #calculate means\n",
    "    for i in range(len(X_train)):\n",
    "        for j in range(len(X_train[0])):\n",
    "            if not math.isnan(X_train[i][j]):\n",
    "                if y[i]==-1:\n",
    "                    means_classed[j][0]+=X_train[i][j]\n",
    "                    ncount_classed[j][0]+=1\n",
    "                elif y[i]==1:\n",
    "                    means_classed[j][1]+=X_train[i][j]\n",
    "                    ncount_classed[j][1]+=1\n",
    "    for i in range(8):\n",
    "        for j in range(2):\n",
    "            if ncount_classed[i][j] != 0:\n",
    "                means_classed[i][j] = means_classed[i][j]/ncount_classed[i][j]\n",
    "\n",
    "    #update values\n",
    "    for i in range(n):\n",
    "        for j in range(len(X[0])):\n",
    "            if math.isnan(X[i][j]):\n",
    "                if y[i]==-1:\n",
    "                    X[i][j] = means_classed[j][0]\n",
    "                elif y[i]==1:\n",
    "                    X[i][j] = means_classed[j][1]\n",
    "\n",
    "    #4.\n",
    "    if len(X_train) > 0:\n",
    "        mat1 = np.array([[1]]*len(X_train))\n",
    "        X_til= np.concatenate((mat1, X_train), axis=1)\n",
    "        w = ana_sol(X_til,y_train)\n",
    "    else:\n",
    "        w = [1]*9\n",
    "\n",
    "    #6.\n",
    "    j_train, j_test = 0,0\n",
    "    for i in range(len(y_train)):\n",
    "        if y_train[i] != classify(w,X_train[i]):\n",
    "            j_train+=1\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i] != classify(w,X_test[i]):\n",
    "            j_test+=1\n",
    "    err_train, err_test = 0,0\n",
    "    if len(y_train)>0: \n",
    "        err_train = j_train*1.0/len(y_train)\n",
    "    if len(y_test)>0:\n",
    "        err_test = j_test*1.0/len(y_test)\n",
    "    return err_train, err_test\n",
    "\n",
    "#print calc_errs(80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error rate on the training set is 0.23289902280130292 and the one on the test set is 0.2012987012987013. They are now a bit further apart and the test error is again lower than the training error, which we did not expect for similar reasons as explained in the previous exercise. In comparison to the previous exercise, the results are alot worse, since all errors (in-sample, out-of-sample, generalization) increased. We expected this behaviour, because we used less data for the prediction of the NaN values, for which reason the quality of the data should be worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\"> **QUESTION BLOCK 6:**\n",
    "<ol>\n",
    "<li> Repeat the process in block 5 changing the percentage of the data for training and testing. Plot a graph with the training and test error rates for each splitting percentage point. Comment the results.</li>\n",
    "<li> Add to the plot the upper bound on the generalization error using the equation of the slides for VC dimension equal to $d + 1$. Discuss the result.</li>\n",
    "<li> How many samples does the bound predict in order to have 1% error deviation with a confidence of 95%? And with confidence 50%? What about 5% and 10% error deviation with 95% confidence? Comment the behavior according to your observations.</li>\n",
    "</ol>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEZCAYAAACq1zMoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc8d38bfd10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def vcdim(n, d, delta):\n",
    "    mue = 1-delta\n",
    "    try: \n",
    "        return math.sqrt((d*(math.log(2*n/d)+1)+math.log(2.0/delta))/(2*n))\n",
    "        #return = math.sqrt((1.0/n)*(d*(math.log(2*n/d)+1)-math.log(mue/4.0)))\n",
    "    except (ValueError, ZeroDivisionError):\n",
    "        return 0\n",
    "        #return vcdim(n,d,delta+0.01)\n",
    "\n",
    "err_train, err_test = [0]*101, [0]*101\n",
    "x_plot = range(101)\n",
    "for i in x_plot:\n",
    "    err_train[i], err_test[i] = calc_errs(i)\n",
    "    \n",
    "#1.\n",
    "plt.title('errors: blue=training, red=test, green=training+generalization')\n",
    "plt.xlabel('split percentage')\n",
    "plt.ylabel('error rate')\n",
    "plt.plot(x_plot, err_train, 'b')\n",
    "plt.hold('on')\n",
    "plt.plot(x_plot, err_test, 'r')\n",
    "#2.\n",
    "plt.hold('on')\n",
    "err_gen = [0]*101\n",
    "for i in x_plot:\n",
    "    err_gen[i] = vcdim(i*0.01*n, 9, 0.05)\n",
    "    \n",
    "err_trgen = [0]*101\n",
    "for i in x_plot:\n",
    "    err_trgen[i] = err_train[i] + err_gen[i]\n",
    "plt.plot(x_plot, err_trgen, 'g--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1.  The curves of both error rates have the form that we expected, but it is interesting that the training error is greater than the test error for splits over 20%. This means that the generalization error is becoming very low very quickly. The reason for this should, again, be the low complexity of the classifier.\n",
    "2. As it seems the curve of the generalization upper bound is almost parallel to the curve of the test error, but it is a lot higher overall, for which reason it is not very helpful for predicting the actual error rate.\n",
    "3. According to WolframAlpha, the amount of samples needed would be: 739229 for 1% error deviation with 95% confidence, 726963 for 1% error deviation with 50% confidence, 22711 for 5% error deviation with 95% confidence and 4996 for 10% error deviation with 95% confidence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
