{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 - A practical case\n",
    "## I. Abstract\n",
    "todo\n",
    "## II. Introduction\n",
    "todo\n",
    "## III. Specification of the problem and organization of the software\n",
    "<p>We are the new data analyst in a Telecom company. Our first project is to try to reduce the\n",
    "churn of our new service. Modeling churn means to understand what keeps the customer engaged\n",
    "to our product. As an analyst, our goal is to predict or describe the churn rate i.e. the rate at\n",
    "which customer leave or cease the subscription to a service. Its value lies in the fact that engaging\n",
    "new customers is often more costly than retaining existing ones. For that reason subscription\n",
    "business-based companies usually have proactive policies towards customer retention.</p>\n",
    "<p>In this case study, we aim at building a machine learning model for customer churn prediction\n",
    "on data from a Telecom company. Each row on the dataset represents a subscribing telephone\n",
    "customer. Each column contains customer attributes such as phone number, call minutes used\n",
    "during different times of day, charges incurred for services, lifetime account duration, and whether\n",
    "or not the customer is still a customer. Data can be found in the file: churn.csv.</p>\n",
    "\n",
    "The complete set of attributes is the following:\n",
    "<ul>\n",
    "<li>State: categorical, for the 50 states and the District of Columbia</li>\n",
    "<li>Account length: integer-valued, how long an account has been active</li>\n",
    "<li>Area code: categorical</li>\n",
    "<li>Phone number: customer ID</li>\n",
    "<li>International Plan: binary feature, yes or no</li>\n",
    "<li>VoiceMail Plan: binary feature, yes or no</li>\n",
    "<li>Number of voice mail messages: integer-valued</li>\n",
    "<li>Total day minutes: continuous, minutes customer used service during the day</li>\n",
    "<li>Total day calls: integer-valued</li>\n",
    "<li>Total day charge: continuous</li>\n",
    "<li>Total evening minutes: continuous, minutes customer used service during the evening</li>\n",
    "<li>Total evening calls: integer-valued</li>\n",
    "<li>Total evening charge: continuous</li>\n",
    "<li>Total night minutes: continuous, minutes customer used service during the night</li>\n",
    "<li>Total night calls: integer-valued</li>\n",
    "<li>Total night charge: continuous</li>\n",
    "<li>Total international minutes: continuous, minutes customer used service to make international\n",
    "calls</li>\n",
    "<li>Total international calls: integer-valued</li>\n",
    "<li>Total international charge: continuous</li>\n",
    "<li>Number of calls to customer service: integer-valued</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Experiments\n",
    "\n",
    "### Code Design\n",
    "All classifiers must be coded from scratch in python, no toolkits are allowed to be used\n",
    "except for numpy and CVX (http://www.cvxpy.org/). All classifiers have to be fully integrable\n",
    "in the scikit-learn (sklearn) ecosystem, i.e. inherit from BaseEstimator and implement the basic\n",
    "methods, fit, predict, score, ... You can check how to integrate your classifier in http://scikit-learn.org/stable/developers/contributing.html#coding-guidelines. We do not need to pass all unit\n",
    "testing but at least we would like to use several of the nice functions such as gridsearchCV, etc\n",
    "with your estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**WP 1**\n",
    "<ol>\n",
    "<li> Read the dataset ‘churn.csv‘ and split it into features and labels</li> \n",
    "<li> Convert data to floating point numbers </li>\n",
    "<li> Drop all not useful features </li>\n",
    "</ol>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\" style=\"border-radius:10px\"> **HINT:** You may want to consider\n",
    "pandas library for cleaning purposes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the dataset ‘churn.csv‘\n",
    "data = pd.read_csv('churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean data\n",
    "# drop features: state, account length, area code, phone\n",
    "to_drop = ['State', 'Account Length', 'Area Code', 'Phone']\n",
    "clean_data = data.drop(to_drop, axis=1)\n",
    "\n",
    "boolean_cols = [\"Int'l Plan\",\"VMail Plan\", 'Churn?']\n",
    "for col in boolean_cols:\n",
    "    # convert categorical data into numeric data\n",
    "    clean_data[col] = clean_data[col].astype('category').cat.codes\n",
    "\n",
    "# convert to float data\n",
    "clean_data = clean_data.astype(np.float)\n",
    "# split it into features and labels\n",
    "X, y = clean_data.drop(['Churn?'], axis=1), clean_data['Churn?']\n",
    "\n",
    "# set y values as 1 and -1\n",
    "y[y == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO: can we use this function? \n",
    "# normalize data\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# TODO: can we use this function? IMPORTANT: scikit-learn >= 18.0 \n",
    "# split into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**WP 2**\n",
    "<ol>\n",
    "<li> Try solving the problem using sklearn techniques.</li> \n",
    "</ol>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (n_estimators=10): 0.940\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "print('Random Forest (n_estimators=10): {:.3f}'.format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**WP 3**\n",
    "<ol>\n",
    "<li> Implement your classifier.</li> \n",
    "</ol>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# include BaseEstimator class and for generating random numbers according to scikit guidelines\n",
    "import sklearn\n",
    "\n",
    "# Calculate the Gini index for a split dataset\n",
    "def weighted_gini_index(groups):\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        labels = group[:, -1]\n",
    "        if len(labels):\n",
    "            class_values, counts = np.unique(labels, return_counts=True)\n",
    "            class_prob = counts / float(len(labels))\n",
    "            weight = 1.0 * len(group) / len(groups)\n",
    "            gini += weight * (1.0 - np.dot(class_prob, class_prob))\n",
    "    return gini\n",
    "\n",
    "# Calculate the Entropy for a split dataset\n",
    "def weighted_entropy(groups):\n",
    "    entropy = 0.0\n",
    "    for group in groups:\n",
    "        labels = group[:, -1]\n",
    "        if len(labels):\n",
    "            class_values, counts = np.unique(labels, return_counts=True)\n",
    "            class_prob = counts / float(len(labels))\n",
    "            weight = 1.0 * len(group) / len(groups)\n",
    "            entropy += -weight * np.dot(class_prob, np.log2(class_prob))\n",
    "    return entropy\n",
    "\n",
    "class MyDecisionTreeClassifier(sklearn.base.BaseEstimator):\n",
    "    def __init__(self, max_features=None, max_depth=None, min_samples_leaf=1, boostrap=False, criterion='gini', random_state=None):\n",
    "        self.max_features = max_features\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.boostrap = boostrap\n",
    "        self.criterion = criterion\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        #internal representation\n",
    "        self._num_features = None\n",
    "        self._score_f = None\n",
    "        self._tree = None\n",
    "                \n",
    "    def fit(self, X, y):\n",
    "        # generate random numbers according to scikit guidelines        \n",
    "        self.random_state = sklearn.utils.check_random_state(self.random_state)\n",
    "        \n",
    "        if self.criterion == 'gini':\n",
    "            self._score_f = weighted_gini_index\n",
    "        elif self.criterion == 'entropy':\n",
    "            self._score_f = weighted_entropy\n",
    "        else:\n",
    "            self._score_f = weighted_gini_index\n",
    "        \n",
    "        # default: all of features\n",
    "        self._num_features = len(X[0]) if self.max_features is None else self.max_features\n",
    "        \n",
    "        dataset = np.column_stack((X, y))\n",
    "\n",
    "        # TODO: when boostrap is false, should data be shuffled?\n",
    "        if self.boostrap:\n",
    "            sample = self._subsample(dataset)\n",
    "\n",
    "        self._tree = self._build_internal(sample)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._rec_predict(self._tree, row) for row in X])\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        return np.sum(self.predict(X) == y) / float(len(y))\n",
    "    \n",
    "    # internal methods\n",
    "    \n",
    "    # Split a dataset based on an attribute and an attribute value\n",
    "    def _test_split(self, index, value, dataset):\n",
    "        feature = dataset[:, index]\n",
    "        return dataset[feature < value], dataset[feature >= value]\n",
    "\n",
    "    # Select the best split point for a dataset\n",
    "    def _get_split(self, dataset):\n",
    "        b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "\n",
    "        features = self.random_state.choice(range(len(dataset[0])-1), self._num_features, replace=False)\n",
    "\n",
    "        for index in features:\n",
    "            for row in dataset:\n",
    "                groups = self._test_split(index, row[index], dataset)\n",
    "                score = self._score_f(groups)\n",
    "                if score < b_score:\n",
    "                    b_index, b_value, b_score, b_groups = index, row[index], score, groups\n",
    "        return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "\n",
    "    # Create a terminal node value\n",
    "    def _to_terminal(self, group):\n",
    "        outcomes = [row[-1] for row in group]\n",
    "        return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "    # Create child splits for a node or make terminal\n",
    "    def _split(self, node, depth):\n",
    "        left, right = node['groups']\n",
    "        del(node['groups'])\n",
    "        # check for a no split\n",
    "        if len(left) == 0 or len(right) == 0:\n",
    "            node['left'] = node['right'] = self._to_terminal(np.vstack([left, right]))\n",
    "            return\n",
    "        # check for max depth\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            node['left'], node['right'] = self._to_terminal(left), self._to_terminal(right)\n",
    "            return\n",
    "        # process left child\n",
    "        if len(left) <= self.min_samples_leaf:\n",
    "            node['left'] = self._to_terminal(left)\n",
    "        else:\n",
    "            node['left'] = self._get_split(left)\n",
    "            self._split(node['left'], depth+1)\n",
    "        # process right child\n",
    "        if len(right) <= self.min_samples_leaf:\n",
    "            node['right'] = self._to_terminal(right)\n",
    "        else:\n",
    "            node['right'] = self._get_split(right)\n",
    "            self._split(node['right'], depth+1)\n",
    "\n",
    "    # Make a prediction with a decision tree\n",
    "    def _rec_predict(self, node, row):\n",
    "        if row[node['index']] < node['value']:\n",
    "            if isinstance(node['left'], dict):\n",
    "                return self._rec_predict(node['left'], row)\n",
    "            else:\n",
    "                return node['left']\n",
    "        else:\n",
    "            if isinstance(node['right'], dict):\n",
    "                return self._rec_predict(node['right'], row)\n",
    "            else:\n",
    "                return node['right']\n",
    "\n",
    "    # Create a random subsample from the dataset with replacement\n",
    "    def _subsample(self, dataset):\n",
    "        n_sample = len(dataset)\n",
    "        index = self.random_state.choice(range(n_sample), n_sample, replace=self.boostrap)\n",
    "        sample = dataset[index]\n",
    "        return sample \n",
    "\n",
    "    # Build a decision tree\n",
    "    def _build_internal(self, train):\n",
    "        root = self._get_split(train)\n",
    "        self._split(root, 1)\n",
    "        return root\n",
    "    \n",
    "class MyRandomForestClassifier(sklearn.base.BaseEstimator):\n",
    "\n",
    "    def __init__(self, n_estimators=10, max_features=None, max_depth=None, min_samples_leaf=1, boostrap=True, criterion='gini', random_state=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.boostrap = boostrap\n",
    "        self.criterion = criterion\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # internal representation\n",
    "        self._estimators = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # generate random numbers according to scikit guidelines\n",
    "        self.random_state = sklearn.utils.check_random_state(self.random_state)        \n",
    "\n",
    "        # default: sqrt of the total number of features\n",
    "        num_features = int(np.sqrt(len(X[0]))) if self.max_features is None else self.max_features\n",
    "            \n",
    "        params = dict(max_depth=self.max_depth, min_samples_leaf=self.min_samples_leaf, \n",
    "                      max_features=num_features, boostrap=self.boostrap, criterion=self.criterion,\n",
    "                      random_state=self.random_state) \n",
    "\n",
    "        self._estimators = [MyDecisionTreeClassifier(**params).fit(X, y) for t in range(self.n_estimators)]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):        \n",
    "        return np.array([self._bagging_predict(sample) for sample in X]) \n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.sum(self.predict(X) == y) / float(len(y))\n",
    "    \n",
    "    # Make a prediction with a list of bagged trees\n",
    "    def _bagging_predict(self, sample):\n",
    "        class_values, counts = np.unique([tree.predict([sample]) for tree in self._estimators], return_counts=True)\n",
    "        return class_values[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**WP 4**\n",
    "<ol>\n",
    "<li> Test your classifier.</li> \n",
    "</ol>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%time my_clf = MyRandomForestClassifier(n_estimators=10, criterion='entropy').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 105 ms\n",
      "Confusion Matrix testing dataset\n",
      "\n",
      "            True class\n",
      "              0   1\n",
      "Predicted  0 564  35 \n",
      "class      1 2  66       \n",
      "\n",
      "Our Random Forest -- Stats\n",
      "accuracy:0.945 precision:0.971 recall:0.653\n"
     ]
    }
   ],
   "source": [
    "%time predictions = my_clf.predict(X_test)\n",
    "\n",
    "print('Confusion Matrix testing dataset')\n",
    "text = \"\"\"\n",
    "            True class\n",
    "              0   1\n",
    "Predicted  0 {tn}  {fn} \n",
    "class      1 {fp}  {tp}       \n",
    "\"\"\"\n",
    "\n",
    "POS, NEG = 1, -1\n",
    "TP = np.sum( y_test[predictions == POS] == POS )\n",
    "TN = np.sum( y_test[predictions == NEG] == NEG )\n",
    "FP = np.sum( y_test[predictions == POS] == NEG )\n",
    "FN = np.sum( y_test[predictions == NEG] == POS )\n",
    "\n",
    "print(text.format(tn=TN, tp=TP, fn=FN, fp=FP))\n",
    "accuracy = 1. * (TN + TP) / (TN + TP + FN + FP)\n",
    "precision = 1. * TP / (TP + FP)\n",
    "recall = 1. * TP / (TP + FN)\n",
    "print('Our Random Forest -- Stats') \n",
    "print('accuracy:{:.3f} precision:{:.3f} recall:{:.3f}'.format(accuracy, precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**WP 5**\n",
    "<ol>\n",
    "<li> Evaluate your code comparing the results with a couple of techniques from sklearn.</li> \n",
    "</ol>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier -- Testing accuracy\n",
      "Wall time: 4 ms\n",
      "Wall time: 84 ms\n",
      "Nearest Neighbors: 0.894\n",
      "\n",
      "Wall time: 112 ms\n",
      "Wall time: 17 ms\n",
      "Linear SVM: 0.849\n",
      "\n",
      "Wall time: 125 ms\n",
      "Wall time: 22 ms\n",
      "RBF SVM: 0.930\n",
      "\n",
      "Wall time: 30 ms\n",
      "Wall time: 1e+03 µs\n",
      "Decision Tree: 0.921\n",
      "\n",
      "Wall time: 102 ms\n",
      "Wall time: 8 ms\n",
      "Random Forest: 0.946\n",
      "\n",
      "Wall time: 843 ms\n",
      "Wall time: 2 ms\n",
      "Neural Net: 0.939\n",
      "\n",
      "Wall time: 295 ms\n",
      "Wall time: 7 ms\n",
      "AdaBoost: 0.877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    SVC(kernel=\"linear\"),\n",
    "    SVC(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(criterion='entropy'),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "]\n",
    "\n",
    "# iterate over classifiers\n",
    "print('Classifier -- Testing accuracy')\n",
    "for name, clf in zip(names, classifiers):\n",
    "    %time clf.fit(X_train, y_train)\n",
    "    %time accuracy = clf.score(X_test, y_test)\n",
    "    print('{}: {:.3f}\\n'.format(name, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is an unbalanced problem and our goal is to miss as few as possible\n",
    "churn clients. For each predicted churn client the company will spend resources to try to\n",
    "keep him engaged.\n",
    "Consider that a given client yields a benefit of 100 euros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**WP 6**\n",
    "<ol>\n",
    "<li> Propose a solution for different costs (in euros) for regaining a client</li> \n",
    "<li> Look for the most profitable retention campaign (How much can be invest in\n",
    "retention?)</li> \n",
    "\n",
    "</ol>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**Results**\n",
    "<ol>\n",
    "<li> Table comparing performances. This includes, for instance, error rate and standard deviation. </li> \n",
    "<li> Graphical presentation of the information in the previous table. </li> \n",
    "<li> Graphical representation of the benefit vs the cost of the retention campaign. </li>\n",
    "</ol>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Conclusions and Future Work\n",
    "todo\n",
    "## VI. Bibliography\n",
    "todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
