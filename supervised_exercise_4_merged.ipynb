{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 - A practical case\n",
    "## I. Abstract\n",
    "todo\n",
    "## II. Introduction\n",
    "<div>\n",
    "<p></p>\n",
    "<p style='text-align:justify'>\n",
    "    Now a day, we can find a high competition between most of the companies from different fields in the business world, this have been increase through the years making companies difficult to reatain the costumers in their services and moreover attract new costumers as a potential clients. Considering the fact that engaging\n",
    "new customers is often more costly and difficult than retaining existing ones, heads of the companies concerned about this issue are investing in costumer satisfaction polices that can allow to improve the revenue in their business.\n",
    "<br></br>\n",
    "Consequently, the strategical and analyst deparment of each company have found that one of the most important factors that assure the steady-state level of customers in a business is the <strong>churn-rate</strong> [1]. This factor is also known as attrition rate and is related to the number of individuals that are quiting their serivices from a determinated company in a certain period. \n",
    "</p>\n",
    "<p style=\"text-align:justify\">In this project we are aiming to determine the churn-rate from a dataset took related to a mobile and communications company <strong>Telecom</strong> in which we have 3333 samples that stand for different costumers in a certain period, and 21 features that are composed by (4 subject attributes, 16 call measurments and their respective churn-rate).\n",
    "<br></br>\n",
    "<br></br>\n",
    "An importan aspect that it has to be considered is that the domain values from each variable/feature quite different in most of the variables, this will lead to the problem due to there will not be and equal contribution for each one of the variables to the data analysis i.e. features with higher values in their scales will influence more in the model when performing the classification task. For that reason it is important to consider a data standarization and prevent this problem. Typical data standardization procedures equalize the range and/or data variability.\n",
    "\n",
    "The most useful techineques to standarize the data are the following [2]:\n",
    "<p></p>\n",
    "<ol>\n",
    "    <li>Ponderated standarization (0-1 scaling): $$\\frac{F_i - min(F_i)}{max(F_i) - min(F_i)}$$</li>\n",
    "    <li>Dividing each value by the range: $$\\frac{F_i}{max(F_i) - min(F_i)}$$</li>\n",
    "    <li>Normal standarization (z-score scaling): $$\\frac{F_i - \\mu_i}{\\sigma_i}$$</li>\n",
    "    <li>Dividing each value by the standard deviation: $$\\frac{F_i}{\\sigma_i}$$</li>\n",
    "    \n",
    "</ol>\n",
    "</p>\n",
    "<p style=\"text-align:justify\">\n",
    "Furthermore, we can implemented the data standarization reviewed before considering all the values from the feature or just the values from the feature related to the certain values of other features i.e. considering the normalization of all the values from the total_day_mis, in which state is equal to AZ. This may give an more accurate measurment at data normalization of each feature.\n",
    "<p></p>\n",
    "To Continue, theres is another important section of the project related to the metholodogy that is going to be implemented in order to solve the classification problem from the perspective of the supervised learning. The solving methodologies that are suitable for this task are th following:\n",
    "\n",
    "<ol>\n",
    "    <li>Adaboost</li>\n",
    "    <li>Random Forest</li>\n",
    "    <li>QP solver for SVM considering Stochastic Gradient Descent as an Optimiztion Algorithm </li>\n",
    "</ol>\n",
    "</p>\n",
    "\n",
    "<h3>Adaboost</h3>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "A machine learning algorithm based on a supervised approach invented by Yoav Freund a computer science researcher from University of California San Diego in 1995 and redesigned by Robert Shapire a computer science researcher from Princeton University in 1999. The aim of this pattern recognition algorithm is to create a robust classifier based on the linear combination of weak or simple classifiers, as a result of this linear combination there is a non-linear machine learning algorithm, this technique and concepts have been implemented in the well-known \"Viola & Jones\" that is applied in face recognition application in the computer vision area [3, 4].\n",
    "<p></p>\n",
    "\n",
    "<p>Mathematical representation: $$H({x_i}) = {\\displaystyle\\sum_{j=1}^{T} \\alpha_j * h_j({x_i})}$$</p>\n",
    "\n",
    "<p>Where:</p>\n",
    "<p>$$\n",
    "\\begin{array}{ll}\n",
    "H_j({x_i}): classification of the strong classifier\\\\\n",
    "x_i: the i^th observation\\\\ \n",
    "T: number of stumps\\\\ \n",
    "\\alpha_j: weight of the j^th stumps in linear combination\\\\\n",
    "h_j({x_i}): classification of a stump (weak classifier)\\\\\n",
    "\\end{array} \n",
    "$$</p>\n",
    "\n",
    "<h4>Characteristics: </h4>\n",
    "<ul>\n",
    "    <li>A robust non-linear classifier.</li>\n",
    "    <li>Has a low generalization error.</li>\n",
    "    <li>Is quite robust to overfitting.</li>\n",
    "    <li>Is intuitive to implement.</li>\n",
    "</ul>\n",
    "\n",
    "<h4>Advantages: </h4>\n",
    "<ul>\n",
    "    <li>Find the best or most representative features, can be used as a principal feature selection.</li>\n",
    "    <li>Find the best thresholds stumps (classifiers).</li>\n",
    "    <li>Find a strong nad robust classifier.</li>\n",
    "</ul>\n",
    "\n",
    "<h4>Considerations: </h4>\n",
    "<ul>\n",
    "    <li>Each weak classifier has to have at least 50% of accuracy, which means that requires to have a higher than 0% alpha, in other words it has to have a relevant weight to be considered as part of the strong classifier.</li>\n",
    "</ul>\n",
    "\n",
    "<h4>Posible stumps or weak classifiers: </h4>\n",
    "<ul>\n",
    "    <li>Decision stump, implementing logistic regression (single axis space partitions).</li>\n",
    "    <li>Decision tree (hierarchical partitions).</li>\n",
    "    <li>Random forest (hierarchical partitions).</li>\n",
    "    <li>Multi-layer perceptron (non-linear approximators).</li>\n",
    "    <li>Support Vector Machines (linear classifier & Kernell).</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<h3>Random Forests</h3>\n",
    "<h4>Characteristics: </h4>\n",
    "<h4>Advantages: </h4>\n",
    "<h4>Considerations: </h4>\n",
    "<h4>Posible stumps or weak classifiers: </h4>\n",
    "\n",
    "</p>\n",
    "\n",
    "</div>\n",
    "## III. Specification of the problem and organization of the software\n",
    "<p>We are the new data analyst in a Telecom company. Our first project is to try to reduce the\n",
    "churn of our new service. Modeling churn means to understand what keeps the customer engaged\n",
    "to our product. As an analyst, our goal is to predict or describe the churn rate i.e. the rate at\n",
    "which customer leave or cease the subscription to a service. Its value lies in the fact that engaging\n",
    "new customers is often more costly than retaining existing ones. For that reason subscription\n",
    "business-based companies usually have proactive policies towards customer retention.</p>\n",
    "<p>In this case study, we aim at building a machine learning model for customer churn prediction\n",
    "on data from a Telecom company. Each row on the dataset represents a subscribing telephone\n",
    "customer. Each column contains customer attributes such as phone number, call minutes used\n",
    "during different times of day, charges incurred for services, lifetime account duration, and whether\n",
    "or not the customer is still a customer. Data can be found in the file: churn.csv.</p>\n",
    "\n",
    "The complete set of attributes is the following:\n",
    "<ul>\n",
    "<li>State: categorical, for the 50 states and the District of Columbia</li>\n",
    "<li>Account length: integer-valued, how long an account has been active</li>\n",
    "<li>Area code: categorical</li>\n",
    "<li>Phone number: customer ID</li>\n",
    "<li>International Plan: binary feature, yes or no</li>\n",
    "<li>VoiceMail Plan: binary feature, yes or no</li>\n",
    "<li>Number of voice mail messages: integer-valued</li>\n",
    "<li>Total day minutes: continuous, minutes customer used service during the day</li>\n",
    "<li>Total day calls: integer-valued</li>\n",
    "<li>Total day charge: continuous</li>\n",
    "<li>Total evening minutes: continuous, minutes customer used service during the evening</li>\n",
    "<li>Total evening calls: integer-valued</li>\n",
    "<li>Total evening charge: continuous</li>\n",
    "<li>Total night minutes: continuous, minutes customer used service during the night</li>\n",
    "<li>Total night calls: integer-valued</li>\n",
    "<li>Total night charge: continuous</li>\n",
    "<li>Total international minutes: continuous, minutes customer used service to make international\n",
    "calls</li>\n",
    "<li>Total international calls: integer-valued</li>\n",
    "<li>Total international charge: continuous</li>\n",
    "<li>Number of calls to customer service: integer-valued</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Experiments\n",
    "\n",
    "### Code Design\n",
    "All classifiers must be coded from scratch in python, no toolkits are allowed to be used\n",
    "except for numpy and CVX (http://www.cvxpy.org/). All classifiers have to be fully integrable\n",
    "in the scikit-learn (sklearn) ecosystem, i.e. inherit from BaseEstimator and implement the basic\n",
    "methods, fit, predict, score, ... You can check how to integrate your classifier in http://scikit-learn.org/stable/developers/contributing.html#coding-guidelines. We do not need to pass all unit\n",
    "testing but at least we would like to use several of the nice functions such as gridsearchCV, etc\n",
    "with your estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**WP 1**\n",
    "<ol>\n",
    "<li> Read the dataset ‘churn.csv‘ and split it into features and labels</li> \n",
    "<li> Convert data to floating point numbers </li>\n",
    "<li> Drop all not useful features </li>\n",
    "</ol>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\" style=\"border-radius:10px\"> **HINT:** You may want to consider\n",
    "pandas library for cleaning purposes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Read the dataset ‘churn.csv‘\n",
    "data = pd.read_csv('churn.csv')\n",
    "\n",
    "# clean data\n",
    "# drop features: state, account length, area code, phone\n",
    "to_drop = ['State', 'Account Length', 'Area Code', 'Phone']\n",
    "clean_data = data.drop(to_drop, axis=1)\n",
    "\n",
    "boolean_cols = [\"Int'l Plan\",\"VMail Plan\", 'Churn?']\n",
    "for col in boolean_cols:\n",
    "    # convert categorical data into numeric data\n",
    "    clean_data[col] = clean_data[col].astype('category').cat.codes\n",
    "\n",
    "# convert to float data\n",
    "clean_data = clean_data.astype(np.float)\n",
    "# split it into features and labels\n",
    "X, y = clean_data.drop(['Churn?'], axis=1), clean_data['Churn?']\n",
    "\n",
    "# set y values as 1 and -1\n",
    "y[y == 0] = -1\n",
    "\n",
    "#Normalization of the data via Ponderated Normalization\n",
    "for i in range(X.shape[1]):\n",
    "    X.ix[:,i] = (X.ix[:,i] - X.ix[:,i].min()) / (X.ix[:,i].max() - X.ix[:,i].min());\n",
    "\n",
    "test_size=.25\n",
    "cut = int(len(X) * (1.0-test_size))\n",
    "X = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = X[:cut], X[cut:], y[:cut], y[cut:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**WP 2**\n",
    "<ol>\n",
    "<li> Try solving the problem using sklearn techniques.</li> \n",
    "</ol>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores of sklearn techniques:\n",
      "\n",
      "Nearest Neighbours: 0.892086330935\n",
      "Linear SVM: 0.836930455635\n",
      "RBF SVM: 0.906474820144\n",
      "Gaussian Process: 0.932853717026\n",
      "Decision Tree: 0.932853717026\n",
      "Random Forest: 0.840527577938\n",
      "Neural Network: 0.83932853717\n",
      "AdaBoost: 0.880095923261\n",
      "Naive Bayes: 0.863309352518\n",
      "Quadratic Discriminant Analysis: 0.858513189448\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "#define classifiers\n",
    "classifiers = [\n",
    "    (KNeighborsClassifier(3), \"Nearest Neighbours\"),\n",
    "    (SVC(kernel=\"linear\", C=0.025), \"Linear SVM\"),\n",
    "    (SVC(gamma=2, C=1), \"RBF SVM\"),\n",
    "    (GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True), \"Gaussian Process\"),\n",
    "    (DecisionTreeClassifier(max_depth=5), \"Decision Tree\"),\n",
    "    (RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1), \"Random Forest\"),\n",
    "    (MLPClassifier(alpha=1), \"Neural Network\"),\n",
    "    (AdaBoostClassifier(), \"AdaBoost\"),\n",
    "    (GaussianNB(), \"Naive Bayes\"),\n",
    "    (QuadraticDiscriminantAnalysis(), \"Quadratic Discriminant Analysis\")]\n",
    "\n",
    "#Train and test classifiers                                                         \n",
    "print \"Scores of sklearn techniques:\\n\"\n",
    "for csf in classifiers:\n",
    "    csf[0].fit(X_train,y_train)\n",
    "    print csf[1] + \": \" + str(csf[0].score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**WP 3**\n",
    "<ol>\n",
    "<li> Implement your classifier.</li> \n",
    "</ol>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">***3.1 - Adaboost***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin;\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted;\n",
    "from sklearn.utils.multiclass import unique_labels;\n",
    "from sklearn.metrics import euclidean_distances;\n",
    "from sklearn import linear_model;\n",
    "from sklearn import metrics;\n",
    "\n",
    "#This ADABOOST implementation considers binary classes {-1, 1}\n",
    "\n",
    "class myAdaboostClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, T=5, demo_param='demo'):\n",
    "        self.demo_param = demo_param;\n",
    "        self.T = T; #Number of ADABOOST stumps\n",
    "        self.adaboost_stumps = np.zeros((T,5)); #ADABOOST stumps(feature-best classifier,threshold,alpha,log_reg coef)\n",
    "        self.stump_generator = linear_model.LogisticRegression();  #ADABOOST Stump Threshold Generator - LogReg approach : ax + b = 0; => x or th = -b / a;\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        self.classes_ = unique_labels(y)\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        \n",
    "        W = (1.0 / len(self.y_)) * np.ones(self.y_.shape[0]);\n",
    "\n",
    "        for t in range(self.T):\n",
    "\n",
    "                lessMisclassifiedSample = [];\n",
    "                lessClassEntropy = [];\n",
    "                minErr = 1;\n",
    "                bestFeat = 0;\n",
    "                bestCoef = 0;\n",
    "                bestAlpha = 1;\n",
    "                bestThreshold = 0;\n",
    "\n",
    "                for i in range(self.X_.shape[1]):\n",
    "\n",
    "                        feature_i = np.transpose(np.atleast_2d(self.X_[:,i]));\n",
    "                        self.stump_generator.fit(feature_i,self.y_);\n",
    "                        threshold = -1 * (self.stump_generator.intercept_ / self.stump_generator.coef_);\n",
    "                        \n",
    "                        y_e = self.stump_generator.predict(feature_i);\n",
    "                        \n",
    "                        class_entropy = y_e*self.y_;\n",
    "                        missclassified_samples = [1 if ce == -1 else 0 for ce in class_entropy];\n",
    "                        J_err = np.dot(missclassified_samples,W);\n",
    "                        alpha_f = (1.0/2) * np.log((1-J_err) / J_err);\n",
    "\n",
    "                        if J_err < minErr:\n",
    "\n",
    "                                minErr = J_err;\n",
    "                                bestFeat = i;\n",
    "                                bestCoef = self.stump_generator.coef_;\n",
    "                                bestAlpha = alpha_f;\n",
    "                                bestThreshold = threshold;\n",
    "                                lessMisclassifiedSample = missclassified_samples;\n",
    "                                lessClassEntropy = class_entropy;\n",
    "                                \n",
    "                \n",
    "                self.adaboost_stumps[t,:] = np.array([bestFeat,bestThreshold,bestAlpha,bestCoef,minErr]);\n",
    "\n",
    "                for i in range(self.y_.shape[0]):\n",
    "                        W[i] = W[i] * np.exp(bestAlpha*(-1 * lessClassEntropy[i]));\n",
    "\n",
    "                Wnorm = np.sum(W);\n",
    "                W = W / Wnorm;\n",
    "        \n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        check_is_fitted(self, ['X_', 'y_']);\n",
    "        \n",
    "        X = check_array(X);\n",
    "        \n",
    "        y_e = np.zeros(X.shape[0]);\n",
    "        \n",
    "        higherClass = 1;\n",
    "        lowerClass = -1;\n",
    "                \n",
    "        for i in range(X.shape[0]):\n",
    "            \n",
    "            stumpsClassLinearCom = 0;\n",
    "            \n",
    "            x_class = 0;\n",
    "            \n",
    "            for t in range(self.T):\n",
    "                \n",
    "                if  X[i,self.adaboost_stumps[t,0]] * self.adaboost_stumps[t,3] < 0:\n",
    "                    \n",
    "                    if X[i,self.adaboost_stumps[t,0]] > self.adaboost_stumps[t,1]:\n",
    "                        x_class = lowerClass;\n",
    "                    else:\n",
    "                        x_class = higherClass;\n",
    "                        \n",
    "                else:\n",
    "                    \n",
    "                    if X[i,self.adaboost_stumps[t,0]] > self.adaboost_stumps[t,1]:\n",
    "                        x_class = higherClass;\n",
    "                    else:\n",
    "                        x_class = lowerClass;\n",
    "                     \n",
    "                stumpsClassLinearCom = stumpsClassLinearCom + x_class * self.adaboost_stumps[t,2];\n",
    "                    \n",
    "            if stumpsClassLinearCom < 0:\n",
    "                    y_e[i] = lowerClass;\n",
    "            else:\n",
    "                    y_e[i] = higherClass;\n",
    "        \n",
    "        \n",
    "        return y_e;\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        return sum(self.predict(X) == y) / len(y);\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
    "        return {\"alpha\": self.alpha, \"recursive\": self.recursive}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            self.setattr(parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">***3.2 - Random Forest***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# include BaseEstimator class and for generating random numbers according to scikit guidelines\n",
    "import sklearn\n",
    "\n",
    "# Calculate the Gini index for a split dataset\n",
    "def weighted_gini_index(groups):\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        labels = group[:, -1]\n",
    "        if len(labels):\n",
    "            class_values, counts = np.unique(labels, return_counts=True)\n",
    "            class_prob = counts / float(len(labels))\n",
    "            weight = 1.0 * len(group) / len(groups)\n",
    "            gini += weight * (1.0 - np.dot(class_prob, class_prob))\n",
    "    return gini\n",
    "\n",
    "# Calculate the Entropy for a split dataset\n",
    "def weighted_entropy(groups):\n",
    "    entropy = 0.0\n",
    "    for group in groups:\n",
    "        labels = group[:, -1]\n",
    "        if len(labels):\n",
    "            class_values, counts = np.unique(labels, return_counts=True)\n",
    "            class_prob = counts / float(len(labels))\n",
    "            weight = 1.0 * len(group) / len(groups)\n",
    "            entropy += -weight * np.dot(class_prob, np.log2(class_prob))\n",
    "    return entropy\n",
    "\n",
    "class MyDecisionTreeClassifier(sklearn.base.BaseEstimator):\n",
    "    def __init__(self, max_features=None, max_depth=None, min_samples_leaf=1, boostrap=False, criterion='gini', random_state=None):\n",
    "        self.max_features = max_features\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.boostrap = boostrap\n",
    "        self.criterion = criterion\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        #internal representation\n",
    "        self._num_features = None\n",
    "        self._score_f = None\n",
    "        self._tree = None\n",
    "                \n",
    "    def fit(self, X, y):\n",
    "        # generate random numbers according to scikit guidelines        \n",
    "        self.random_state = sklearn.utils.check_random_state(self.random_state)\n",
    "        \n",
    "        if self.criterion == 'gini':\n",
    "            self._score_f = weighted_gini_index\n",
    "        elif self.criterion == 'entropy':\n",
    "            self._score_f = weighted_entropy\n",
    "        else:\n",
    "            self._score_f = weighted_gini_index\n",
    "        \n",
    "        # default: all of features\n",
    "        self._num_features = len(X[0]) if self.max_features is None else self.max_features\n",
    "        \n",
    "        dataset = np.column_stack((X, y))\n",
    "\n",
    "        # TODO: when boostrap is false, should data be shuffled?\n",
    "        if self.boostrap:\n",
    "            sample = self._subsample(dataset)\n",
    "\n",
    "        self._tree = self._build_internal(sample)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._rec_predict(self._tree, row) for row in X])\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        return np.sum(self.predict(X) == y) / float(len(y))\n",
    "    \n",
    "    # internal methods\n",
    "    \n",
    "    # Split a dataset based on an attribute and an attribute value\n",
    "    def _test_split(self, index, value, dataset):\n",
    "        feature = dataset[:, index]\n",
    "        return dataset[feature < value], dataset[feature >= value]\n",
    "\n",
    "    # Select the best split point for a dataset\n",
    "    def _get_split(self, dataset):\n",
    "        b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "\n",
    "        features = self.random_state.choice(range(len(dataset[0])-1), self._num_features, replace=False)\n",
    "\n",
    "        for index in features:\n",
    "            for row in dataset:\n",
    "                groups = self._test_split(index, row[index], dataset)\n",
    "                score = self._score_f(groups)\n",
    "                if score < b_score:\n",
    "                    b_index, b_value, b_score, b_groups = index, row[index], score, groups\n",
    "        return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "\n",
    "    # Create a terminal node value\n",
    "    def _to_terminal(self, group):\n",
    "        outcomes = [row[-1] for row in group]\n",
    "        return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "    # Create child splits for a node or make terminal\n",
    "    def _split(self, node, depth):\n",
    "        left, right = node['groups']\n",
    "        del(node['groups'])\n",
    "        # check for a no split\n",
    "        if len(left) == 0 or len(right) == 0:\n",
    "            node['left'] = node['right'] = self._to_terminal(np.vstack([left, right]))\n",
    "            return\n",
    "        # check for max depth\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            node['left'], node['right'] = self._to_terminal(left), self._to_terminal(right)\n",
    "            return\n",
    "        # process left child\n",
    "        if len(left) <= self.min_samples_leaf:\n",
    "            node['left'] = self._to_terminal(left)\n",
    "        else:\n",
    "            node['left'] = self._get_split(left)\n",
    "            self._split(node['left'], depth+1)\n",
    "        # process right child\n",
    "        if len(right) <= self.min_samples_leaf:\n",
    "            node['right'] = self._to_terminal(right)\n",
    "        else:\n",
    "            node['right'] = self._get_split(right)\n",
    "            self._split(node['right'], depth+1)\n",
    "\n",
    "    # Make a prediction with a decision tree\n",
    "    def _rec_predict(self, node, row):\n",
    "        if row[node['index']] < node['value']:\n",
    "            if isinstance(node['left'], dict):\n",
    "                return self._rec_predict(node['left'], row)\n",
    "            else:\n",
    "                return node['left']\n",
    "        else:\n",
    "            if isinstance(node['right'], dict):\n",
    "                return self._rec_predict(node['right'], row)\n",
    "            else:\n",
    "                return node['right']\n",
    "\n",
    "    # Create a random subsample from the dataset with replacement\n",
    "    def _subsample(self, dataset):\n",
    "        n_sample = len(dataset)\n",
    "        index = self.random_state.choice(range(n_sample), n_sample, replace=self.boostrap)\n",
    "        sample = dataset[index]\n",
    "        return sample \n",
    "\n",
    "    # Build a decision tree\n",
    "    def _build_internal(self, train):\n",
    "        root = self._get_split(train)\n",
    "        self._split(root, 1)\n",
    "        return root\n",
    "    \n",
    "class MyRandomForestClassifier(sklearn.base.BaseEstimator):\n",
    "\n",
    "    def __init__(self, n_estimators=10, max_features=None, max_depth=None, min_samples_leaf=1, boostrap=True, criterion='gini', random_state=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.boostrap = boostrap\n",
    "        self.criterion = criterion\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # internal representation\n",
    "        self._estimators = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # generate random numbers according to scikit guidelines\n",
    "        self.random_state = sklearn.utils.check_random_state(self.random_state)        \n",
    "\n",
    "        # default: sqrt of the total number of features\n",
    "        num_features = int(np.sqrt(len(X[0]))) if self.max_features is None else self.max_features\n",
    "            \n",
    "        params = dict(max_depth=self.max_depth, min_samples_leaf=self.min_samples_leaf, \n",
    "                      max_features=num_features, boostrap=self.boostrap, criterion=self.criterion,\n",
    "                      random_state=self.random_state) \n",
    "\n",
    "        self._estimators = [MyDecisionTreeClassifier(**params).fit(X, y) for t in range(self.n_estimators)]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):        \n",
    "        return np.array([self._bagging_predict(sample) for sample in X]) \n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.sum(self.predict(X) == y) / float(len(y))\n",
    "    \n",
    "    # Make a prediction with a list of bagged trees\n",
    "    def _bagging_predict(self, sample):\n",
    "        class_values, counts = np.unique([tree.predict([sample]) for tree in self._estimators], return_counts=True)\n",
    "        return class_values[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**WP 4**\n",
    "<ol>\n",
    "<li> Test your classifier.</li> \n",
    "</ol>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADABOOST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:93: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/felix/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:102: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            True class\n",
      "              0   1\n",
      "Predicted  0 697  134 \n",
      "class      1 1  2       \n",
      "\n",
      "Accuracy:  0.838129496403\n",
      "Precision:  0.666666666667\n",
      "Recall:  0.0147058823529\n",
      "\n",
      "\n",
      "\n",
      "RANDOM FOREST\n",
      "\n",
      "            True class\n",
      "              0   1\n",
      "Predicted  0 692  51 \n",
      "class      1 6  85       \n",
      "\n",
      "Accuracy:  0.931654676259\n",
      "Precision:  0.934065934066\n",
      "Recall:  0.625\n"
     ]
    }
   ],
   "source": [
    "POS, NEG = 1, -1\n",
    "\n",
    "conf_matrix = \"\"\"\n",
    "            True class\n",
    "              0   1\n",
    "Predicted  0 {tn}  {fn} \n",
    "class      1 {fp}  {tp}       \n",
    "\"\"\"\n",
    "\n",
    "#prints confustion matrix, accuracy, precision and recall for the given predictions\n",
    "def print_stats(predictions):\n",
    "    TP = np.sum( y_test[predictions == POS] == POS )\n",
    "    TN = np.sum( y_test[predictions == NEG] == NEG )\n",
    "    FP = np.sum( y_test[predictions == POS] == NEG )\n",
    "    FN = np.sum( y_test[predictions == NEG] == POS )\n",
    "\n",
    "    print(conf_matrix.format(tn=TN, tp=TP, fn=FN, fp=FP))\n",
    "    print \"Accuracy: \", 1. * (TN + TP) / (TN + TP + FN + FP)\n",
    "    print \"Precision: \", 1. * TP / (TP + FP)\n",
    "    print \"Recall: \", 1. * TP / (TP + FN)\n",
    "\n",
    "  \n",
    "#testing adaboost\n",
    "print('ADABOOST')\n",
    "ada = myAdaboostClassifier(5).fit(X_train,y_train)\n",
    "print_stats(ada.predict(X_test))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "#testing random forest\n",
    "print('RANDOM FOREST')\n",
    "rnf = MyRandomForestClassifier(n_estimators=10, criterion='entropy').fit(X_train, y_train)\n",
    "predictions = rnf.predict(X_test)\n",
    "print_stats(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**WP 5**\n",
    "<ol>\n",
    "<li> Evaluate your code comparing the results with a couple of techniques from sklearn.</li> \n",
    "</ol>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let us now compare our results with the results of sklearn techniques observed in WP2:\n",
    "<ul>\n",
    "<li>Nearest Neighbours: 0.892086330935</li>\n",
    "<li>Linear SVM: 0.836930455635</li>\n",
    "<li>RBF SVM: 0.906474820144</li>\n",
    "<li>Gaussian Process: 0.932853717026</li>\n",
    "<li>Decision Tree: 0.932853717026</li>\n",
    "<li>Random Forest: 0.840527577938</li>\n",
    "<li>Neural Network: 0.83932853717</li>\n",
    "<li>AdaBoost: 0.880095923261</li>\n",
    "<li>Naive Bayes: 0.863309352518</li>\n",
    "<li>Quadratic Discriminant Analysis: 0.858513189448</li>\n",
    "</ul>\n",
    "\n",
    "As we can see, the scores of the sklearn techniques vary strongly, so we will only compare our classifier with the highest scoring techniques.\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is an unbalanced problem and our goal is to miss as few as possible\n",
    "churn clients. For each predicted churn client the company will spend resources to try to\n",
    "keep him engaged.\n",
    "Consider that a given client yields a benefit of 100 euros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**WP 6**\n",
    "<ol>\n",
    "<li> Propose a solution for different costs (in euros) for regaining a client</li> \n",
    "<li> Look for the most profitable retention campaign (How much can be invest in\n",
    "retention?)</li> \n",
    "\n",
    "</ol>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790.0\n"
     ]
    }
   ],
   "source": [
    "#static\n",
    "\n",
    "#financial benefit of preventing a churn\n",
    "ben = 100\n",
    "\n",
    "TP = np.sum( y_test[predictions == POS] == POS )\n",
    "FP = np.sum( y_test[predictions == POS] == NEG )\n",
    "\n",
    "\n",
    "#dynamic\n",
    "\n",
    "#churn prevention rate after actions are performed\n",
    "cpr = 0.2\n",
    "\n",
    "#cost for performing certain actions on potential churners\n",
    "c = 10\n",
    "\n",
    "\n",
    "benefit = ben*cpr*TP\n",
    "cost = c*(TP+FP)\n",
    "profit = benefit - cost\n",
    "\n",
    "print profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**Results**\n",
    "<ol>\n",
    "<li> Table comparing performances. This includes, for instance, error rate and standard deviation. </li> \n",
    "<li> Graphical presentation of the information in the previous table. </li> \n",
    "<li> Graphical representation of the benefit vs the cost of the retention campaign. </li>\n",
    "</ol>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Conclusions and Future Work\n",
    "<div style=\"text-align:justify\">\n",
    "<p></p>\n",
    "<p>After defining properly the project goals, having an accurate representation of the data by performing data standarization, reviewing all the scikit-learn literature pre-developed methodlogies and selecting the most fittable algorithm to perform churn-classification, we come to the conclusion of..</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "## VI. Bibliography\n",
    "<div style=\"text-align:justify\">\n",
    "    <p></p>\n",
    "    <ol>\n",
    "        <li>Wikipedia. (2016, December). Churn rate [Online]. Available: https://en.wikipedia.org/wiki/Churn_rate</li>\n",
    "        <li>BioMedWare. (2011, November). Data Standarization [Online]. Availabe: https://www.biomedware.com/files/documentation/boundaryseer/Preparing_data/How_to_standardize_variables.htm </li>\n",
    "        <li>Robert E. Schapire. A Brief Introduction to Boosting. <i>Sixteenth International Joint Conference on Artificial Intelligence</i>. New Jersey, U.S.A. 1999.</li>\n",
    "        <li>Robert E. Schapire. A Brief Explaining ADABOOST. <i>Princeton  University,  Dept.  of  Computer  Science</i>. New Jersey, U.S.A. 2012.</li>\n",
    "        <li>Wikipedia. (2016, December). Random Forests [Online]. Available: https://en.wikipedia.org/wiki/Random_forest</li>\n",
    "        <li>Wikipedia. (2016, December). Decision Trees [Online]. Available: https://en.wikipedia.org/wiki/Decision_tree_learning</li>\n",
    "        <li>Wikipedia. (2016, December). ID3 [Online]. Available: https://en.wikipedia.org/wiki/ID3_algorithm </li>\n",
    "    </ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
